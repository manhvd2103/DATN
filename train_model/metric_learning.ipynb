{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20961,"status":"ok","timestamp":1656643721151,"user":{"displayName":"vu manh","userId":"16167959225577149137"},"user_tz":-420},"id":"9NpQDpYVkgLu","outputId":"fa905916-c683-488a-de9b-98c93afdc532"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"cell_type":"code","source":["!nvidia-smi -L\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q8NGEnhe7PRL","executionInfo":{"status":"ok","timestamp":1656681857294,"user_tz":-420,"elapsed":337,"user":{"displayName":"vu manh","userId":"16167959225577149137"}},"outputId":"25c7a6dd-baff-4100-8153-fc260194f7fa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU 0: Tesla T4 (UUID: GPU-578476da-1f50-a56f-cf8d-7524ffb9fdc1)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dlg8mFLRky8X"},"outputs":[],"source":["!unzip \"/content/drive/MyDrive/DATN/data/roi_extraction.zip\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TZwiC7oWWmkF"},"outputs":[],"source":["import os\n","from shutil import copy, rmtree\n","import random\n","def mk_file(file_path: str):\n","    if os.path.exists(file_path):\n","        rmtree(file_path)\n","    os.makedirs(file_path)\n","\n","def main():\n","    random.seed(0)\n","\n","    split_rate = 0.2\n","\n","    cwd = os.getcwd()\n","    data_root = os.path.join(cwd, \"/content/data\")\n","    origin_palmpPrint_path = os.path.join(\"/content/roi_extraction\")\n","    assert os.path.exists(origin_palmpPrint_path), \"path '{}' does not exist.\".format(origin_palmpPrint_path)\n","\n","    palmPrint_class = [cla for cla in os.listdir(origin_palmpPrint_path)]\n","\n","    train_root = os.path.join(data_root, \"train\")\n","    mk_file(train_root)\n","    for cla in palmPrint_class:\n","        mk_file(os.path.join(train_root, cla))\n","\n","    val_root = os.path.join(data_root, \"val\")\n","    mk_file(val_root)\n","    for cla in palmPrint_class:\n","        mk_file(os.path.join(val_root, cla))\n","\n","    for cla in palmPrint_class:\n","        cla_path = os.path.join(origin_palmpPrint_path, cla)\n","        images = os.listdir(cla_path)\n","        num = len(images)\n","        eval_index = random.sample(images, k=int(num*split_rate))\n","        for index, image in enumerate(images):\n","            if image in eval_index:\n","                image_path = os.path.join(cla_path, image)\n","                new_path = os.path.join(val_root, cla)\n","                copy(image_path, new_path)\n","            else:\n","                image_path = os.path.join(cla_path, image)\n","                new_path = os.path.join(train_root, cla)\n","                copy(image_path, new_path)\n","            print(\"\\r[{}] processing [{}/{}]\".format(cla, index+1, num), end=\"\")  # processing bar\n","        print()\n","\n","    print(\"processing done\")\n","\n","if __name__ == '__main__':\n","    main()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0wScf4djiRWQ"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from sklearn.preprocessing import label_binarize\n","from typing import Tuple\n","\n","def binarize_and_smooth_labels(labels: torch.Tensor,\n","                               n_classes: int,\n","                               smoothing_factor: float,\n","                               device: torch.device\n","                               ) -> torch.FloatTensor:\n","    labels = labels.cpu().numpy()\n","    labels = label_binarize(labels, classes=range(0, n_classes))\n","    labels = labels * (1 - smoothing_factor)\n","    labels[labels == 0] = smoothing_factor / (n_classes - 1)\n","    labels = torch.from_numpy(labels).float().to(device)\n","    return labels\n","\n","\n","class ProxyNCALoss(nn.Module):\n","    def __init__(self,\n","                 n_classes: int,\n","                 embedding_size: int,\n","                 embedding_scale: float,\n","                 proxy_scale: float,\n","                 smoothing_factor: float,\n","                 device: torch.device,\n","                 ):\n","        super().__init__()\n","\n","        self.device: torch.device = device\n","        self.n_classes: int = n_classes\n","        self.embedding_size: int = embedding_size\n","        self.embedding_scale: float = embedding_scale\n","\n","        self.proxies = nn.Parameter(torch.randn(n_classes, embedding_size) / 8).to(device)\n","        self.proxy_scale: float = proxy_scale\n","        self.smoothing_factor: float = smoothing_factor\n","\n","    def forward(self, embeddings: torch.Tensor, labels: torch.Tensor) -> Tuple[torch.Tensor, int]:\n","        # embeddings shape: [batch_size * embedding_size]\n","        embeddings = embeddings * self.embedding_scale\n","        # proxies shape: [n_classes * embedding_size]\n","        proxies: torch.Tensor = F.normalize(self.proxies, p=2, dim=1) * self.proxy_scale\n","        # distances shape: [batch_size * n_classes]\n","        distances: torch.Tensor = torch.cdist(embeddings, proxies).square()\n","\n","        # labels shape: [batch_size * n_classes]\n","        labels = binarize_and_smooth_labels(labels, self.n_classes, self.smoothing_factor, self.device)\n","        proxy_nca_loss: torch.Tensor = (-labels * F.log_softmax(-distances, dim=1)).sum(dim=1)\n","\n","        return proxy_nca_loss.mean(), 0\n","\n","\n","class ProxyAnchorLoss(nn.Module):\n","    def __init__(self, n_classes: int, embedding_size: int, margin: float, alpha: float, device: torch.device):\n","        super().__init__()\n","\n","        self.device: torch.device = device\n","        self.n_classes: int = n_classes\n","        self.embedding_size: int = embedding_size\n","\n","        # shape: [n_classes * embedding_size]\n","        self.proxies = nn.Parameter(torch.rand(n_classes, embedding_size)).to(device)\n","        nn.init.kaiming_normal_(self.proxies, mode=\"fan_out\")\n","\n","        self.margin: float = margin\n","        self.alpha: float = alpha\n","\n","    def forward(self, embeddings: torch.Tensor, labels: torch.Tensor) -> Tuple[torch.Tensor, int]:\n","        # proxies shape: [n_classes * embedding_size]\n","        proxies: torch.Tensor = F.normalize(self.proxies, p=2, dim=1)\n","\n","        # cosine_distances shape: [batch_size * n_classes]\n","        cosine_distances = F.linear(embeddings, proxies)\n","        # positive_exp shape: [batch_size * n_classes]\n","        positive_exp = torch.exp(-self.alpha * (cosine_distances - self.margin))\n","        # negative_exp shape: [batch_size * n_classes]\n","        negative_exp = torch.exp(self.alpha * (cosine_distances + self.margin))\n","\n","        # shape: [batch_size * n_classes]\n","        labels_onehot: torch.Tensor = binarize_and_smooth_labels(\n","            labels, self.n_classes, smoothing_factor=0, device=self.device\n","        )\n","        # Indices of all positive proxies in batch, shape [n_classes]\n","        distinct_proxies_indices = torch.nonzero(labels_onehot.sum(dim=0) != 0).squeeze(dim=1)\n","        n_distinct_proxies: int = len(distinct_proxies_indices)\n","\n","        # positive_distances: distance from a pariticular proxy to all positive samples in a batch\n","        # shape: [n_classes]\n","        sum_positive_distances = torch.where(\n","            labels_onehot == 1, positive_exp, torch.zeros_like(positive_exp)\n","        ).sum(dim=0)\n","\n","        # negative distances: distance from a particular proxy to all negative samples in a batch\n","        # shape: [n_classes]\n","        sum_negative_distances = torch.where(\n","            labels_onehot == 0, negative_exp, torch.zeros_like(negative_exp)\n","        ).sum(dim=0)\n","\n","        positive_term = torch.log(1 + sum_positive_distances).sum() / n_distinct_proxies\n","        negative_term = torch.log(1 + sum_negative_distances).sum() / self.n_classes\n","        proxy_anchor_loss: torch.Tensor = positive_term + negative_term\n","        return proxy_anchor_loss, 0\n","\n","class SoftTripleLoss(nn.Module):\n","    def __init__(self,\n","                 n_classes: int,\n","                 embedding_size: int,\n","                 n_centers_per_class: int,\n","                 lambda_: float,\n","                 gamma: float,\n","                 tau: float,\n","                 margin: float,\n","                 device: torch.device\n","                 ):\n","        super().__init__()\n","\n","        self.device: torch.device = device\n","        self.n_classes: int = n_classes\n","        self.embedding_size: int = embedding_size\n","        self.n_centers_per_class: int = n_centers_per_class\n","\n","        self.lambda_: float = lambda_\n","        self.gamma: float = gamma\n","        self.tau: float = tau\n","        self.margin: float = margin\n","\n","        # Each class has n centers\n","        self.centers: torch.Tensor = nn.Parameter(\n","            torch.rand(embedding_size, n_centers_per_class * n_classes)\n","        ).to(device)\n","        nn.init.kaiming_uniform_(self.centers, a=5**0.5)\n","\n","        # weight for regularization term\n","        self.weight: torch.Tensor = torch.zeros(\n","            n_classes * n_centers_per_class,\n","            n_classes * n_centers_per_class,\n","            dtype=torch.long\n","        ).to(device)\n","\n","        for i in range(n_classes):\n","            for j in range(n_centers_per_class):\n","                self.weight[\n","                    i * n_centers_per_class + j,\n","                    i * n_centers_per_class + j + 1:(i + 1) * n_centers_per_class\n","                ] = 1\n","\n","    def forward(self, embeddings: torch.Tensor, labels: torch.Tensor) -> Tuple[torch.Tensor, int]:\n","        # labels shape: [n_classes]\n","        labels = labels.to(self.device)\n","        # centers shape: [embedding_size * (n_classes * n_centers_per_class)]\n","        centers = F.normalize(self.centers, p=2, dim=0)\n","\n","        # Distance from each embedding to all centers\n","        # distances shape: [batch_size * n_classes * n_centers_per_class]\n","        distances = embeddings.matmul(centers).reshape(-1, self.n_classes, self.n_centers_per_class)\n","\n","        # probabilities shape: [batch_size * n_classes * n_centers_per_class]\n","        probabilities = F.softmax(distances * self.gamma, dim=2)\n","\n","        # Distance from each embedding to its TRUE center in a particular class\n","        # will depend on the distances betweeen it and all centers in that class.\n","        # relaxed_distances shape: [batch_size * n_classes]\n","        relaxed_distances = torch.sum(probabilities * distances, dim=2)\n","\n","        margin = torch.zeros_like(relaxed_distances)\n","        margin[torch.arange(0, len(embeddings)), labels] = self.margin\n","        soft_triple_loss = F.cross_entropy(self.lambda_ * (relaxed_distances - margin), labels)\n","\n","        if self.tau > 0 and self.n_centers_per_class > 1:\n","            # Distances between all pairs of centers\n","            distances_centers = centers.t().matmul(centers)\n","            dominator = torch.sum(torch.sqrt(2.0 + 1e-5 - 2. * distances_centers[self.weight]))\n","            denominator = (self.n_classes * self.n_centers_per_class * (self.n_centers_per_class - 1.))\n","            regularization = dominator / denominator\n","            return soft_triple_loss + self.tau * regularization, 0\n","\n","        else:\n","            return soft_triple_loss, 0\n","\n","class TripletMarginLoss(nn.Module):\n","    def __init__(self, margin=1.0, p=2.0, sampling_type=\"batch_hard_triplets\"):\n","        super().__init__()\n","        self.margin: float = margin\n","        self.p: float = p\n","        self.sampling_type: str = sampling_type\n","\n","    def forward(self, embeddings: torch.Tensor, labels: torch.Tensor) -> Tuple[torch.Tensor, float]:\n","        if self.sampling_type == \"batch_hard_triplets\":\n","            return _batch_hard_triplets_loss(labels, embeddings, self.margin, self.p)\n","        elif self.sampling_type == \"batch_hardest_triplets\":\n","            return _batch_hardest_triplets_loss(labels, embeddings, self.margin, self.p)\n","        else:\n","            raise NotImplementedError(self.sampling_type)\n","\n","\n","def _batch_hard_triplets_loss(labels: torch.Tensor,\n","                              embeddings: torch.Tensor,\n","                              margin: float,\n","                              p: float\n","                              ) -> Tuple[torch.Tensor, float]:\n","\n","    pairwise_distance: torch.Tensor = torch.cdist(embeddings, embeddings, p=p)\n","\n","    anchor_positive_distance: torch.Tensor = pairwise_distance.unsqueeze(2)\n","    anchor_negative_distance: torch.Tensor = pairwise_distance.unsqueeze(1)\n","\n","    # Indexes of all triplets\n","    mask: torch.Tensor = _get_triplet_masks(labels)\n","    # Calucalate triplet loss\n","    triplet_loss: torch.Tensor = mask.float() * (anchor_positive_distance - anchor_negative_distance + margin)\n","\n","    # Remove negative loss (easy triplets)\n","    triplet_loss[triplet_loss < 0] = 0\n","\n","    # Count number of positive triplets (where triplet_loss > 0)\n","    hard_triplets: torch.Tensor = triplet_loss[triplet_loss > 1e-16]\n","    n_hard_triplets: int = hard_triplets.size(0)\n","\n","    # Total triplets (including positive and negative triplet)\n","    n_triplets: int = mask.sum().item()\n","    # Fraction of postive triplets in total\n","    fraction_hard_triplets: float = n_hard_triplets / (n_triplets + 1e-16)\n","\n","    # Get final mean triplet loss over the positive valid triplets\n","    triplet_loss = triplet_loss.sum() / (n_hard_triplets + 1e-16)\n","    return triplet_loss, fraction_hard_triplets\n","\n","\n","def _batch_hardest_triplets_loss(labels: torch.Tensor,\n","                                 embeddings: torch.Tensor,\n","                                 margin: float,\n","                                 p: float\n","                                 ) -> Tuple[torch.Tensor, int]:\n","\n","    pairwise_distance: torch.Tensor = torch.cdist(embeddings, embeddings, p=p)\n","    # Indexes of all triplets\n","    mask_anchor_positive: torch.Tensor = _get_anchor_positive_mask(labels).float()\n","    # Distance between anchors and positives\n","    anchor_positive_distance: torch.Tensor = mask_anchor_positive * pairwise_distance\n","\n","    # Hardest postive for every anchor\n","    hardest_positive_distance, _ = anchor_positive_distance.max(1, keepdim=True)\n","\n","    mask_anchor_negative: torch.Tensor = _get_anchor_negative_mask(labels).float()\n","    # Add max value in each row to invalid negatives\n","    max_anchor_negative_distance, _ = pairwise_distance.max(dim=1, keepdim=True)\n","    anchor_negative_distance = pairwise_distance + max_anchor_negative_distance * (1.0 - mask_anchor_negative)\n","\n","    # Hardest negative for every anchor\n","    hardest_negative_distance, _ = anchor_negative_distance.min(dim=1, keepdim=True)\n","\n","    triplet_loss: torch.Tensor = hardest_positive_distance - hardest_negative_distance + margin\n","    triplet_loss[triplet_loss < 0] = 0\n","    triplet_loss = triplet_loss.mean()\n","    return triplet_loss, 0\n","\n","\n","def _get_triplet_masks(labels: torch.Tensor) -> torch.Tensor:\n","    # indices_equal is a square matrix, 1 in the diagonal and 0 everywhere else\n","    indices_equal: torch.Tensor = torch.eye(labels.size(0), dtype=torch.bool, device=labels.device)\n","    # indices_not_equal is inversed of indices_equal, 0 in the diagonal and 1 everywhere else\n","    indices_not_equal: torch.Tensor = ~indices_equal\n","\n","    # convention: i (anchor index), j (positive index), k (negative index)\n","    i_not_equal_j: torch.Tensor = indices_not_equal.unsqueeze(2)\n","    i_not_equal_k: torch.Tensor = indices_not_equal.unsqueeze(1)\n","    j_not_equal_k: torch.Tensor = indices_not_equal.unsqueeze(0)\n","    # Check that anchor, positive, negative are distince to each other\n","    distinct_indices: torch.Tensor = (i_not_equal_j & i_not_equal_k) & j_not_equal_k\n","\n","    label_equal: torch.Tensor = labels.unsqueeze(0) == labels.unsqueeze(1)\n","    i_equal_j: torch.Tensor = label_equal.unsqueeze(2)\n","    i_equal_k: torch.Tensor = label_equal.unsqueeze(1)\n","\n","    # i_equal_j: indices of anchor-positive pairs\n","    # ~i_equal_k: indices of anchor-negative pairs\n","    # indices of valid triplets\n","    indices_triplets: torch.Tensor = i_equal_j & (~i_equal_k)\n","    # Make sure that anchor, positive, negative are distince to each other\n","    indices_triplets = indices_triplets & distinct_indices\n","    return indices_triplets\n","\n","\n","def _get_anchor_positive_mask(labels: torch.Tensor) -> torch.Tensor:\n","    # Check that i and j are distince\n","    indices_equal: torch.Tensor = torch.eye(labels.size(0), dtype=torch.bool, device=labels.device)\n","    indices_not_equal: torch.Tensor = ~indices_equal\n","\n","    # Check anchor and negative\n","    labels_equal: torch.Tensor = labels.unsqueeze(0) == labels.unsqueeze(1)\n","    return labels_equal & indices_not_equal\n","\n","\n","def _get_anchor_negative_mask(labels: torch.Tensor) -> torch.BoolTensor:\n","    return labels.unsqueeze(0) != labels.unsqueeze(1)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ln6WG3tOiqj_"},"outputs":[],"source":["from typing import Callable, List, Optional\n","\n","import torch\n","from torch import nn, Tensor\n","from torch.nn import functional as F\n","from functools import partial\n","\n","\n","def _make_divisible(ch, divisor=8, min_ch=None):\n","    if min_ch is None:\n","        min_ch = divisor\n","    new_ch = max(min_ch, int(ch + divisor / 2) // divisor * divisor)\n","    # Make sure that round down does not go down by more than 10%.\n","    if new_ch < 0.9 * ch:\n","        new_ch += divisor\n","    return new_ch\n","\n","\n","class ConvBNActivation(nn.Sequential):\n","    def __init__(self,\n","                 in_planes: int,\n","                 out_planes: int,\n","                 kernel_size: int = 3,\n","                 stride: int = 1,\n","                 groups: int = 1,\n","                 norm_layer: Optional[Callable[..., nn.Module]] = None,\n","                 activation_layer: Optional[Callable[..., nn.Module]] = None):\n","        padding = (kernel_size - 1) // 2\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","        if activation_layer is None:\n","            activation_layer = nn.ReLU6\n","        super(ConvBNActivation, self).__init__(nn.Conv2d(in_channels=in_planes,\n","                                                         out_channels=out_planes,\n","                                                         kernel_size=kernel_size,\n","                                                         stride=stride,\n","                                                         padding=padding,\n","                                                         groups=groups,\n","                                                         bias=False),\n","                                               norm_layer(out_planes),\n","                                               activation_layer(inplace=True))\n","\n","\n","class SqueezeExcitation(nn.Module):\n","    def __init__(self, input_c: int, squeeze_factor: int = 4):\n","        super(SqueezeExcitation, self).__init__()\n","        squeeze_c = _make_divisible(input_c // squeeze_factor, 8)\n","        self.fc1 = nn.Conv2d(input_c, squeeze_c, 1)\n","        self.fc2 = nn.Conv2d(squeeze_c, input_c, 1)\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        scale = F.adaptive_avg_pool2d(x, output_size=(1, 1))\n","        scale = self.fc1(scale)\n","        scale = F.relu(scale, inplace=True)\n","        scale = self.fc2(scale)\n","        scale = F.hardsigmoid(scale, inplace=True)\n","        return scale * x\n","\n","\n","class InvertedResidualConfig:\n","    def __init__(self,\n","                 input_c: int,\n","                 kernel: int,\n","                 expanded_c: int,\n","                 out_c: int,\n","                 use_se: bool,\n","                 activation: str,\n","                 stride: int,\n","                 width_multi: float):\n","        self.input_c = self.adjust_channels(input_c, width_multi)\n","        self.kernel = kernel\n","        self.expanded_c = self.adjust_channels(expanded_c, width_multi)\n","        self.out_c = self.adjust_channels(out_c, width_multi)\n","        self.use_se = use_se\n","        self.use_hs = activation == \"HS\"  # whether using h-swish activation\n","        self.stride = stride\n","\n","    @staticmethod\n","    def adjust_channels(channels: int, width_multi: float):\n","        return _make_divisible(channels * width_multi, 8)\n","\n","\n","class InvertedResidual(nn.Module):\n","    def __init__(self,\n","                 cnf: InvertedResidualConfig,\n","                 norm_layer: Callable[..., nn.Module]):\n","        super(InvertedResidual, self).__init__()\n","\n","        if cnf.stride not in [1, 2]:\n","            raise ValueError(\"illegal stride value.\")\n","\n","        self.use_res_connect = (cnf.stride == 1 and cnf.input_c == cnf.out_c)\n","\n","        layers: List[nn.Module] = []\n","        activation_layer = nn.Hardswish if cnf.use_hs else nn.ReLU\n","\n","        # expand\n","        if cnf.expanded_c != cnf.input_c:\n","            layers.append(ConvBNActivation(cnf.input_c,\n","                                           cnf.expanded_c,\n","                                           kernel_size=1,\n","                                           norm_layer=norm_layer,\n","                                           activation_layer=activation_layer))\n","\n","        # depthwise\n","        layers.append(ConvBNActivation(cnf.expanded_c,\n","                                       cnf.expanded_c,\n","                                       kernel_size=cnf.kernel,\n","                                       stride=cnf.stride,\n","                                       groups=cnf.expanded_c,\n","                                       norm_layer=norm_layer,\n","                                       activation_layer=activation_layer))\n","\n","        if cnf.use_se:\n","            layers.append(SqueezeExcitation(cnf.expanded_c))\n","\n","        # project\n","        layers.append(ConvBNActivation(cnf.expanded_c,\n","                                       cnf.out_c,\n","                                       kernel_size=1,\n","                                       norm_layer=norm_layer,\n","                                       activation_layer=nn.Identity))\n","\n","        self.block = nn.Sequential(*layers)\n","        self.out_channels = cnf.out_c\n","        self.is_strided = cnf.stride > 1\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        result = self.block(x)\n","        if self.use_res_connect:\n","            result += x\n","\n","        return result\n","\n","\n","class MobileNetV3(nn.Module):\n","    def __init__(self,\n","                 inverted_residual_setting: List[InvertedResidualConfig],\n","                 last_channel: int,\n","                 num_classes: int = 1000,\n","                 block: Optional[Callable[..., nn.Module]] = None,\n","                 norm_layer: Optional[Callable[..., nn.Module]] = None):\n","        super(MobileNetV3, self).__init__()\n","\n","        if not inverted_residual_setting:\n","            raise ValueError(\"The inverted_residual_setting should not be empty.\")\n","        elif not (isinstance(inverted_residual_setting, List) and\n","                  all([isinstance(s, InvertedResidualConfig) for s in inverted_residual_setting])):\n","            raise TypeError(\"The inverted_residual_setting should be List[InvertedResidualConfig]\")\n","\n","        if block is None:\n","            block = InvertedResidual\n","\n","        if norm_layer is None:\n","            norm_layer = partial(nn.BatchNorm2d, eps=0.001, momentum=0.01)\n","\n","        layers: List[nn.Module] = []\n","\n","        # building first layer\n","        firstconv_output_c = inverted_residual_setting[0].input_c\n","        layers.append(ConvBNActivation(3,\n","                                       firstconv_output_c,\n","                                       kernel_size=3,\n","                                       stride=2,\n","                                       norm_layer=norm_layer,\n","                                       activation_layer=nn.Hardswish))\n","        # building inverted residual blocks\n","        for cnf in inverted_residual_setting:\n","            layers.append(block(cnf, norm_layer))\n","\n","        # building last several layers\n","        lastconv_input_c = inverted_residual_setting[-1].out_c\n","        lastconv_output_c = 6 * lastconv_input_c\n","        layers.append(ConvBNActivation(lastconv_input_c,\n","                                       lastconv_output_c,\n","                                       kernel_size=1,\n","                                       norm_layer=norm_layer,\n","                                       activation_layer=nn.Hardswish))\n","        self.features = nn.Sequential(*layers)\n","        self.avgpool = nn.AdaptiveAvgPool2d(1)\n","        self.classifier = nn.Sequential(nn.Linear(lastconv_output_c, last_channel),\n","                                        nn.Hardswish(inplace=True),\n","                                        nn.Dropout(p=0.2, inplace=True),\n","                                        nn.Linear(last_channel, num_classes))\n","\n","        # initial weights\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n","                if m.bias is not None:\n","                    nn.init.zeros_(m.bias)\n","            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n","                nn.init.ones_(m.weight)\n","                nn.init.zeros_(m.bias)\n","            elif isinstance(m, nn.Linear):\n","                nn.init.normal_(m.weight, 0, 0.01)\n","                nn.init.zeros_(m.bias)\n","\n","    def _forward_impl(self, x: Tensor) -> Tensor:\n","        x = self.features(x)\n","        x = self.avgpool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.classifier(x)\n","\n","        return x\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        return self._forward_impl(x)\n","\n","\n","def mobilenet_v3_large(num_classes: int = 1000,\n","                       reduced_tail: bool = False) -> MobileNetV3:\n","    width_multi = 1.0\n","    bneck_conf = partial(InvertedResidualConfig, width_multi=width_multi)\n","    adjust_channels = partial(InvertedResidualConfig.adjust_channels, width_multi=width_multi)\n","\n","    reduce_divider = 2 if reduced_tail else 1\n","\n","    inverted_residual_setting = [\n","        # input_c, kernel, expanded_c, out_c, use_se, activation, stride\n","        bneck_conf(16, 3, 16, 16, False, \"RE\", 1),\n","        bneck_conf(16, 3, 64, 24, False, \"RE\", 2),  # C1\n","        bneck_conf(24, 3, 72, 24, False, \"RE\", 1),\n","        bneck_conf(24, 5, 72, 40, True, \"RE\", 2),  # C2\n","        bneck_conf(40, 5, 120, 40, True, \"RE\", 1),\n","        bneck_conf(40, 5, 120, 40, True, \"RE\", 1),\n","        bneck_conf(40, 3, 240, 80, False, \"HS\", 2),  # C3\n","        bneck_conf(80, 3, 200, 80, False, \"HS\", 1),\n","        bneck_conf(80, 3, 184, 80, False, \"HS\", 1),\n","        bneck_conf(80, 3, 184, 80, False, \"HS\", 1),\n","        bneck_conf(80, 3, 480, 112, True, \"HS\", 1),\n","        bneck_conf(112, 3, 672, 112, True, \"HS\", 1),\n","        bneck_conf(112, 5, 672, 160 // reduce_divider, True, \"HS\", 2),  # C4\n","        bneck_conf(160 // reduce_divider, 5, 960 // reduce_divider, 160 // reduce_divider, True, \"HS\", 1),\n","        bneck_conf(160 // reduce_divider, 5, 960 // reduce_divider, 160 // reduce_divider, True, \"HS\", 1),\n","    ]\n","    last_channel = adjust_channels(1280 // reduce_divider)  # C5\n","\n","    return MobileNetV3(inverted_residual_setting=inverted_residual_setting,\n","                       last_channel=last_channel,\n","                       num_classes=num_classes)\n","    \n","\n","class Mobilenet_v3_large(nn.Module):\n","\n","    def __init__(self, embedding_size: int, weight_path: str=None):\n","        super().__init__()\n","\n","        model = mobilenet_v3_large()\n","        assert os.path.exists(weight_path), \"Weight_path {} does not exists!\".format(weight_path)\n","        model.load_state_dict(torch.load(weight_path, map_location=\"cpu\"))\n","        # Features extraction layers without the last fully-connected\n","        self.features = nn.Sequential(*list(model.children())[:-1])\n","        # Embeddding layer\n","        self.embedding = nn.Sequential(\n","            nn.Linear(in_features=960, out_features=embedding_size)\n","        )\n","\n","    def forward(self, image: torch.Tensor) -> torch.Tensor:\n","        \n","        embedding: torch.Tensor = self.features(image)\n","        embedding = embedding.flatten(start_dim=1)\n","\n","        embedding: torch.Tensor = self.embedding(embedding)\n","        embedding = F.normalize(embedding, p=2, dim=1)\n","        return embedding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-FbBG1RUi0Fe"},"outputs":[],"source":["import numpy as np\n","from torch.utils.data import Subset\n","from torchvision.datasets import ImageFolder\n","from PIL import ImageFile\n","\n","from itertools import groupby\n","from typing import List, Tuple, Dict\n","\n","\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n","\n","\n","class Dataset(ImageFolder):\n","    def __init__(self, images_dir: str, transform=None):\n","        super().__init__(images_dir, transform=transform)\n","\n","        self.idx_to_class: Dict[int, str] = {\n","            idx: class_name\n","            for class_name, idx in self.class_to_idx.items()\n","        }\n","\n","    def __len__(self) -> int:\n","        return len(self.imgs)\n","\n","    def __getitem__(self, index):\n","        path, target = self.samples[index]\n","        sample = self.loader(path)\n","        if self.transform is not None:\n","            sample = self.transform(sample)\n","\n","        return sample, target\n","\n","\n","def get_subset_from_dataset(dataset: Dataset, n_samples_per_class: int) -> Dataset:\n","    # samples + corresponding indices in database_set\n","    samples: List[Tuple[int, str, int]] = [(i, *sample) for i, sample in enumerate(dataset.samples)]\n","    group_by_class_idx = groupby(samples, key=lambda sample: sample[2])  # group by class_index\n","\n","    indices: List[int] = []\n","    for _, group in group_by_class_idx:\n","        group: List = list(group)\n","        indices_in_same_class, _, _ = zip(*group)\n","        indices_in_same_class: List[int] = np.random.choice(\n","            indices_in_same_class, size=n_samples_per_class, replace=False\n","        ).tolist()\n","        indices.extend(indices_in_same_class)\n","\n","    subset: Dataset = Subset(dataset, indices=indices)\n","    return subset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"siR5khT4jydT"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","\n","import os\n","import datetime\n","import random\n","from typing import List, Dict, Tuple, Any, Union\n","\n","\n","def set_random_seed(seed: int) -> None:\n","    \"\"\"\n","    Set random seed for package random, numpy and pytorch\n","    \"\"\"\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","\n","def get_current_time() -> str:\n","    return datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n","\n","\n","def save_checkpoint(model: nn.Module,\n","                    config: Dict[str, Any],\n","                    current_epoch: int,\n","                    output_dir: str,\n","                    mean_average_precision: float = None,\n","                    ) -> str:\n","\n","    checkpoint_name: str = f\"epoch{current_epoch}\"\n","    if mean_average_precision is not None:\n","        checkpoint_name += f\"-map{mean_average_precision:.2f}\"\n","    checkpoint_name += \".pth\"\n","\n","    checkpoint_path: str = os.path.join(output_dir, checkpoint_name)\n","    torch.save(\n","        {\n","            \"config\": config,\n","            \"model_state_dict\": model.module.state_dict(),\n","        },\n","        checkpoint_path\n","    )\n","    return checkpoint_path\n","\n","\n","def log_embeddings_to_tensorboard(loader: DataLoader,\n","                                  model: nn.Module,\n","                                  device: torch.device,\n","                                  writer: SummaryWriter,\n","                                  tag: str\n","                                  ) -> None:\n","    if tag == \"train\":\n","        if hasattr(loader.sampler, \"sequential_sampling\"):\n","            loader.sampler.sequential_sampling = True\n","    # Calculating embedding of training set for visualization\n","    embeddings, labels = get_embeddings_from_dataloader(loader, model, device)\n","    writer.add_embedding(embeddings, metadata=labels.tolist(), tag=tag)\n","\n","\n","@torch.no_grad()\n","def get_embeddings_from_dataloader(loader: DataLoader,\n","                                   model: nn.Module,\n","                                   device: torch.device,\n","                                   return_numpy_array=False,\n","                                   return_image_paths=False,\n","                                   ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[np.ndarray, np.ndarray]]:\n","    model.eval()\n","\n","    embeddings_ls: List[torch.Tensor] = []\n","    labels_ls: List[torch.Tensor] = []\n","    for images_, labels_ in loader:\n","        images: torch.Tensor = images_.to(device, non_blocking=True)\n","        labels: torch.Tensor = labels_.to(device, non_blocking=True)\n","        embeddings: torch.Tensor = model(images)\n","        embeddings_ls.append(embeddings)\n","        labels_ls.append(labels)\n","\n","    embeddings: torch.Tensor = torch.cat(embeddings_ls, dim=0)  # shape: [N x embedding_size]\n","    labels: torch.Tensor = torch.cat(labels_ls, dim=0)  # shape: [N]\n","\n","    if return_numpy_array:\n","        embeddings = embeddings.cpu().numpy()\n","        labels = labels.cpu().numpy()\n","\n","    if return_image_paths:\n","        images_paths: List[str] = []\n","        for path, _ in loader.dataset.samples:\n","            images_paths.append(path)\n","        return (embeddings, labels, images_paths)\n","\n","    return (embeddings, labels)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-9Ef5mzsjuhg"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","import numpy as np\n","from sklearn.cluster import KMeans\n","from sklearn.metrics.cluster import normalized_mutual_info_score\n","from typing import Tuple, Dict\n","\n","def calculate_all_metrics(model: nn.Module,\n","                          test_loader: DataLoader,\n","                          ref_loader: DataLoader,\n","                          device: torch.device,\n","                          k: Tuple[int, int, int] = (1, 5, 10)\n","                          ) -> Dict[str, float]:\n","\n","    # Calculate all embeddings of training set and test set\n","    embeddings_test, labels_test = get_embeddings_from_dataloader(test_loader, model, device)\n","    embeddings_ref, labels_ref = get_embeddings_from_dataloader(ref_loader, model, device)\n","\n","    # Expand dimension for batch calculating\n","    embeddings_test = embeddings_test.unsqueeze(dim=0)  # [M x K] -> [1 x M x embedding_size]\n","    embeddings_ref = embeddings_ref.unsqueeze(dim=0)  # [N x K] -> [1 x N x embedding_size]\n","    labels_test = labels_test.unsqueeze(dim=1)  # [M] -> [M x 1]\n","\n","    # Pairwise distance of all embeddings between test set and reference set\n","    distances: torch.Tensor = torch.cdist(embeddings_test, embeddings_ref, p=2).squeeze()  # [M x N]\n","\n","    # Calculate precision_at_k on test set with k=1, k=5 and k=10\n","    metrics: Dict[str, float] = {}\n","    for i in k:\n","        metrics[f\"average_precision_at_{i}\"] = calculate_precision_at_k(distances,\n","                                                                        labels_test,\n","                                                                        labels_ref,\n","                                                                        k=i\n","                                                                        )\n","    # Calculate mean average precision (MAP)\n","    mean_average_precision: float = sum(precision_at_k for precision_at_k in metrics.values()) / len(metrics)\n","    metrics[\"mean_average_precision\"] = mean_average_precision\n","\n","    # Calculate top-1 and top-5 and top-10 accuracy\n","    for i in k:\n","        metrics[f\"top_{i}_accuracy\"] = calculate_topk_accuracy(distances,\n","                                                               labels_test,\n","                                                               labels_ref,\n","                                                               top_k=i\n","                                                               )\n","    # Calculate NMI score\n","    n_classes: int = len(test_loader.dataset.classes)\n","    metrics[\"normalized_mutual_information\"] = calculate_normalized_mutual_information(\n","        embeddings_test.squeeze(), labels_test.squeeze(), n_classes\n","    )\n","\n","    return metrics\n","\n","\n","def calculate_precision_at_k(distances: torch.Tensor,\n","                             labels_test: torch.Tensor,\n","                             labels_ref: torch.Tensor,\n","                             k: int\n","                             ) -> float:\n","\n","    _, indices = distances.topk(k=k, dim=1, largest=False)  # indices shape: [M x k]\n","\n","    y_pred = []\n","    for i in range(k):\n","        indices_at_k: torch.Tensor = indices[:, i]  # [M]\n","        y_pred_at_k: torch.Tensor = labels_ref[indices_at_k].unsqueeze(dim=1)  # [M x 1]\n","        y_pred.append(y_pred_at_k)\n","\n","    y_pred: torch.Tensor = torch.hstack(y_pred)  # [M x k]\n","    labels_test = torch.hstack((labels_test,) * k)  # [M x k]\n","\n","    precision_at_k: float = ((y_pred == labels_test).sum(dim=1) / k).mean().item() * 100\n","    return precision_at_k\n","\n","\n","def calculate_topk_accuracy(distances: torch.Tensor,\n","                            labels_test: torch.Tensor,\n","                            labels_ref: torch.Tensor,\n","                            top_k: int\n","                            ) -> float:\n","\n","    _, indices = distances.topk(k=top_k, dim=1, largest=False)  # indices shape: [M x k]\n","\n","    y_pred = []\n","    for i in range(top_k):\n","        indices_at_k: torch.Tensor = indices[:, i]  # [M]\n","        y_pred_at_k: torch.Tensor = labels_ref[indices_at_k].unsqueeze(dim=1)  # [M x 1]\n","        y_pred.append(y_pred_at_k)\n","\n","    y_pred: torch.Tensor = torch.hstack(y_pred)  # [M x k]\n","    labels_test = torch.hstack((labels_test,) * top_k)  # [M x k]\n","\n","    n_predictions: int = y_pred.shape[0]\n","    n_true_predictions: int = ((y_pred == labels_test).sum(dim=1) > 0).sum().item()\n","    topk_accuracy: float = n_true_predictions / n_predictions * 100\n","    return topk_accuracy\n","\n","\n","def calculate_normalized_mutual_information(embeddings: torch.Tensor,\n","                                            labels_test: torch.Tensor,\n","                                            n_classes: int\n","                                            ) -> float:\n","    embeddings = embeddings.cpu().numpy()\n","    y_test: np.ndarray = labels_test.cpu().numpy().astype(np.int)\n","\n","    y_pred: np.ndarray = KMeans(n_clusters=n_classes).fit(embeddings).labels_\n","    NMI_score: float = normalized_mutual_info_score(y_test, y_pred)\n","\n","    return NMI_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ftz5smYEkr8q"},"outputs":[],"source":["import numpy as np\n","import torch\n","from torch.utils.data.sampler import Sampler\n","from collections import defaultdict\n","\n","import random\n","from typing import List, DefaultDict, Dict, Iterator, Any\n","\n","\n","def _create_groups(targets: List[int], samples_per_class: int) -> DefaultDict[int, List[int]]:\n","    \n","    class_to_idxs: DefaultDict[int, List[int]] = defaultdict(list)\n","    for idx, class_idx in enumerate(targets):\n","        class_to_idxs[class_idx].append(idx)\n","\n","    # Get classes that have number of samples less than k\n","    classes_to_extend: List[int] = []\n","    for class_ in class_to_idxs:\n","        if len(class_to_idxs[class_]) < samples_per_class:\n","            classes_to_extend.append(class_)\n","            continue\n","\n","    # For class that have less than k sample, we will extend that class\n","    # by duplicating random samples in class until the class has k samples\n","    for class_ in classes_to_extend:\n","        n_samples: int = len(class_to_idxs[class_])\n","        n_samples_to_extend = samples_per_class - n_samples\n","\n","        random_samples: List[int] = []\n","        for _ in range(n_samples_to_extend):\n","            # Choose a random sample in class to duplicate\n","            random_sample: int = np.random.choice(class_to_idxs[class_])\n","            random_samples.append(random_sample)\n","\n","        # Extend current class to have k samples\n","        class_to_idxs[class_].extend(random_samples)\n","\n","    return class_to_idxs\n","\n","\n","class PKSampler(Sampler):\n","\n","    def __init__(self,\n","                 labels: List[int],\n","                 classes_per_batch: int,\n","                 samples_per_class: int,\n","                 sequential_sampling: bool = False\n","                 ):\n","\n","        self.labels: List[int] = labels\n","        self.classes_per_batch: int = classes_per_batch\n","        self.samples_per_class: int = samples_per_class\n","        self.class_to_idxs: DefaultDict[int, List[int]] = _create_groups(labels, self.samples_per_class)\n","\n","        self.sequential_sampling: bool = sequential_sampling\n","\n","        # Ensures there are enough classes to sample from\n","        if len(self.class_to_idxs) < classes_per_batch:\n","            raise Exception(f\"There are not enough classes to sample.\"\n","                            f\"Got: class_to_idxs={self.class_to_idxs}, \"\n","                            f\"classes_per_batch={classes_per_batch}\"\n","                            )\n","\n","    def __iter__(self) -> Iterator[Any]:\n","        \"\"\"\n","        Return index of images and targets to be sampled\n","        \"\"\"\n","        if not self.sequential_sampling:\n","            # Shuffle sample within classes\n","            for key in self.class_to_idxs:\n","                random.shuffle(self.class_to_idxs[key])\n","\n","            # Keep trach of the number of samples left for each classes\n","            class_to_n_samples: Dict[int, int] = {}\n","            for class_, sample_idxs in self.class_to_idxs.items():\n","                class_to_n_samples[class_] = len(sample_idxs)\n","\n","            while len(class_to_n_samples) >= self.classes_per_batch:\n","                # Select p classes at random from valid remaining classes\n","                classes: List[int] = list(class_to_n_samples.keys())\n","                selected_class_idxs: List[int] = torch.multinomial(\n","                    torch.ones(len(classes)),\n","                    self.classes_per_batch\n","                ).tolist()\n","\n","                for i in selected_class_idxs:\n","                    class_: int = classes[i]\n","                    # List of indexes of samples in a particular class\n","                    sample_idxs: List[int] = self.class_to_idxs[class_]\n","                    for _ in range(self.samples_per_class):\n","                        # Sequentially return an index of a sample in a class\n","                        sample_idx: int = len(sample_idxs) - class_to_n_samples[class_]\n","                        yield sample_idxs[sample_idx]\n","                        class_to_n_samples[class_] -= 1\n","\n","                    # Don't sample from class if it has less than k samples remaning\n","                    if class_to_n_samples[class_] < self.samples_per_class:\n","                        class_to_n_samples.pop(class_)\n","\n","        else:\n","            for i in range(len(self.labels)):\n","                yield i"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Iik64j6Nj46G"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","from torch.optim import Optimizer\n","from torch.utils.data import DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","\n","from tqdm import tqdm\n","import sys\n","import logging\n","from typing import Tuple, Dict, Any, Union\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","def train_one_epoch(model: nn.Module,\n","                    optimizer: Optimizer,\n","                    loss_function: Union[TripletMarginLoss, ProxyNCALoss, ProxyAnchorLoss, SoftTripleLoss],\n","                    train_loader: DataLoader,\n","                    test_loader: DataLoader,\n","                    reference_loader: DataLoader,\n","                    writer: SummaryWriter,\n","                    device: torch.device,\n","                    config: Dict[str, Any],\n","                    checkpoint_dir: str,\n","                    log_frequency: int,\n","                    validate_frequency: int,\n","                    output_dict: Dict[str, Any]\n","                    ) -> Dict[str, Any]:\n","\n","    # Increase number of epochs so far\n","    output_dict[\"current_epoch\"] += 1\n","    current_epoch: int = output_dict[\"current_epoch\"]\n","\n","    running_loss: float = 0.0\n","    running_fraction_hard_triplets: float = 0.0\n","\n","    for images, labels in train_loader:\n","        # Increase number of iterations so far\n","        output_dict[\"current_iter\"] += 1\n","        current_iter: int = output_dict[\"current_iter\"]\n","\n","        # Run validation\n","        if current_iter == 1 or (current_iter % validate_frequency == 0):\n","            metrics: Dict[str, Any] = calculate_all_metrics(model.module, test_loader, reference_loader, device)\n","            log_info_metrics(logger, metrics, current_epoch)\n","\n","            # Log all metrics to tensorboard\n","            for metric_name, value in metrics.items():\n","                writer.add_scalar(f\"test/{metric_name}\", value, current_iter)\n","\n","            # Save checkpoint that has highest MAP\n","            if metrics['mean_average_precision'] > output_dict[\"metrics\"][\"mean_average_precision\"]:\n","                output_dict[\"metrics\"] = metrics\n","                save_checkpoint(\n","                    model,\n","                    config,\n","                    current_epoch,\n","                    current_iter,\n","                    checkpoint_dir,\n","                    metrics['mean_average_precision'],\n","                )\n","\n","        output_batch: Dict[str, Any] = train_one_batch(model, optimizer, loss_function, images, labels, device)\n","        running_loss += output_batch[\"loss\"]\n","        running_fraction_hard_triplets += output_batch[\"fraction_hard_triplets\"]\n","\n","        # Logging to tensorboard\n","        for metric_name, value in output_batch.items():\n","            writer.add_scalar(f\"train/{metric_name}\", value, current_iter)\n","\n","        # Logging to standard output stream\n","        if current_iter % log_frequency == 0:\n","            average_loss: float = running_loss / log_frequency\n","            average_hard_triplets: float = running_fraction_hard_triplets / log_frequency * 100\n","            logger.info(\n","                f\"TRAINING\\t[{current_epoch}|{current_iter}]\\t\"\n","                f\"train_loss: {average_loss:.6f}\\t\"\n","                f\"hard triplets: {average_hard_triplets:.2f}%\"\n","            )\n","            running_loss = 0.0\n","            running_fraction_hard_triplets = 0.0\n","\n","    # Run validation at the final iteration\n","    if output_dict[\"current_epoch\"] == output_dict[\"total_epoch\"]:\n","        metrics: Dict[str, Any] = calculate_all_metrics(model.module, test_loader, reference_loader, device)\n","        log_info_metrics(logger, metrics, current_epoch)\n","\n","        for metric_name, value in metrics.items():\n","            writer.add_scalar(f\"test/{metric_name}\", value, current_iter)\n","\n","        if metrics['mean_average_precision'] > output_dict[\"metrics\"][\"mean_average_precision\"]:\n","            output_dict[\"metrics\"] = metrics\n","            save_checkpoint(\n","                model,\n","                config,\n","                current_epoch,\n","                current_iter,\n","                checkpoint_dir,\n","                metrics['mean_average_precision'],\n","            )\n","    return output_dict\n","\n","\n","def train_one_batch(model: nn.Module,\n","                    optimizer: Optimizer,\n","                    loss_function: Union[TripletMarginLoss, ProxyNCALoss, ProxyAnchorLoss, SoftTripleLoss],\n","                    images: torch.Tensor,\n","                    labels: torch.Tensor,\n","                    device: torch.device,\n","                    ) -> Tuple[float, float]:\n","    model.train()\n","    optimizer.zero_grad()\n","\n","    images: torch.Tensor = images.to(device, non_blocking=True)\n","    labels: torch.Tensor = labels.to(device, non_blocking=True)\n","\n","    embeddings: torch.Tensor = model(images)\n","    loss, fraction_hard_triplets = loss_function(embeddings, labels)\n","\n","    loss.backward()\n","    optimizer.step()\n","\n","    return {\n","        \"loss\": loss.item(),\n","        \"fraction_hard_triplets\": float(fraction_hard_triplets)\n","    }\n","\n","\n","def log_info_metrics(logger, metrics: Dict[str, float], current_epoch: int) -> None:\n","    \"\"\"\n","    Print all metrics to stdout\n","    \"\"\"\n","    logger.info(\"*\" * 130)\n","    logger.info(\n","        f\"VALIDATING\\t[{current_epoch}]\\t\"\n","        f\"MAP: {metrics['mean_average_precision']:.2f}%\\t\"\n","        f\"AP@1: {metrics['average_precision_at_1']:.2f}%\\t\"\n","        f\"AP@5: {metrics['average_precision_at_5']:.2f}%\\t\"\n","        f\"Top-1: {metrics['top_1_accuracy']:.2f}%\\t\"\n","        f\"Top-5: {metrics['top_5_accuracy']:.2f}%\\t\"\n","        f\"NMI: {metrics['normalized_mutual_information']:.2f}\\t\"\n","    )\n","    logger.info(\"*\" * 130)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jpR0XGpxj_CP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"error","timestamp":1656646485743,"user_tz":-420,"elapsed":47776,"user":{"displayName":"vu manh","userId":"16167959225577149137"}},"outputId":"8d957e0b-491d-4d91-8e5c-2272e19a40a5"},"outputs":[{"output_type":"stream","name":"stdout","text":["22-Jul-01 03:34:00  __main__  INFO: Created checkpoint directory at: /content/drive/MyDrive/DATN/save_check_points/2022-07-01_03-34-00\n","22-Jul-01 03:34:00  __main__  INFO: Start training...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:105: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"]},{"output_type":"stream","name":"stdout","text":["22-Jul-01 03:34:47  __main__  INFO: **********************************************************************************************************************************\n","22-Jul-01 03:34:47  __main__  INFO: VALIDATING\t[1]\tMAP: 81.25%\tAP@1: 95.78%\tAP@5: 83.63%\tTop-1: 95.78%\tTop-5: 98.72%\tNMI: 0.89\t\n","22-Jul-01 03:34:47  __main__  INFO: **********************************************************************************************************************************\n"]},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-6dd209c6d430>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-21-6dd209c6d430>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'log_frequency'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'validate_frequency'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0moutput_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         )\n\u001b[1;32m    260\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"DONE TRAINING {config['n_epochs']} epochs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-20-ed924d33ffc4>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, loss_function, train_loader, test_loader, reference_loader, writer, device, config, checkpoint_dir, log_frequency, validate_frequency, output_dict)\u001b[0m\n\u001b[1;32m     59\u001b[0m                     \u001b[0mcurrent_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                     \u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                     \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean_average_precision'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m                 )\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: save_checkpoint() takes from 4 to 5 positional arguments but 6 were given"]}],"source":["from torchvision.transforms.transforms import CenterCrop\n","import torch\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","from torch.optim import RAdam\n","\n","import argparse\n","import logging\n","import json\n","import time\n","import sys\n","import os\n","import yaml\n","from pprint import pformat\n","from typing import Dict, Any\n","\n","CURRENT_TIME: str = get_current_time()\n","\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format='%(asctime)s  %(name)s  %(levelname)s: %(message)s',\n","    datefmt='%y-%b-%d %H:%M:%S',\n","    handlers=[\n","        logging.StreamHandler(sys.stdout),\n","        logging.FileHandler(f\"./{CURRENT_TIME}.txt\", mode=\"w\", encoding=\"utf-8\")\n","    ]\n",")\n","logger = logging.getLogger(__name__)\n","\n","\n","def main():\n","    start = time.time()\n","\n","    # Intialize config\n","    config = {\n","        \"lr\": 0.0001,\n","        \"image_size\": 256,\n","        \"embedding_size\": 128,\n","        \"batch_size\": 48,\n","        \"smoothing_factor\": 0.1,\n","        \"alpha\": 32,\n","        \"embedding_scale\": 1.0,\n","        \"proxy_scale\": 3.0,\n","        \"n_epochs\": 100,\n","        \"loss\": \"soft_triple\",\n","        \"n_samples_per_reference_class\": -1,\n","        \"checkpoint_root_dir\": \"/content/drive/MyDrive/DATN/save_check_points\",\n","        \"log_frequency\": 100,\n","        \"validate_frequency\": 1000,\n","        \"sampling_type\": \"batch_hard_triplets\",\n","        \"samples_per_class\": 4,\n","        \"classes_per_batch\": 12,\n","        \"k_queries\": 3,\n","        \"n_centers_per_class\": 5,\n","        # lambda is for calculating distance probability. See section 3.2 in the paper for more detail.\n","        \"lambda\": 20,\n","        # gamma is for calculating cross entropy loss. See section 3.2 in the paper for more detail.\n","        \"gamma\": 0.1,\n","        # tau is for regularization. See section 3.2 in the paper for more detail.\n","        \"tau\": 0.,\n","        # Margin factor\n","        \"margin\": 0.01,\n","        \"n_workers\": 2\n","    }\n","\n","    # Weight_path\n","    weight_path = \"/content/drive/MyDrive/DATN/mobilenet_v3_large-8738ca79.pth\"\n","    # Train_data_path\n","    train_dir = \"/content/data/train\"\n","    # Val_data_dir\n","    val_dir = \"/content/data/val\"\n","    # Initialize device\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    # Initialize number worker\n","    num_worker = min([os.cpu_count(), config[\"batch_size\"] if config[\"batch_size\"] > 1 else 0, 8])\n","    # Intialize model\n","    model = nn.DataParallel(Mobilenet_v3_large(\n","        embedding_size=config[\"embedding_size\"],\n","        weight_path=weight_path\n","    ))\n","    model = model.to(device)\n","\n","\n","    # Initialize optimizer\n","    optimizer = RAdam(model.parameters(), lr=config[\"lr\"])\n","\n","    # Initialize train transforms\n","    transform_train = transforms.Compose([\n","        transforms.Resize((config[\"image_size\"], config[\"image_size\"])),\n","        transforms.RandomResizedCrop((224, 224)),\n","        transforms.RandomRotation(degrees=30),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n","        transforms.RandomAffine(degrees=5, scale=(0.8, 1.2), translate=(0.2, 0.2)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","    ])\n","\n","\n","    # Initialize training set\n","    train_set = Dataset(train_dir, transform=transform_train)\n","\n","    if config[\"loss\"] == \"triplet_loss\":\n","        # Initialize train loader for triplet loss\n","        batch_size: int = config[\"classes_per_batch\"] * config[\"samples_per_class\"]\n","        train_loader = DataLoader(\n","            train_set,\n","            batch_size,\n","            sampler=PKSampler(\n","                train_set.targets,\n","                config[\"classes_per_batch\"],\n","                config[\"samples_per_class\"]\n","            ),\n","            shuffle=False,\n","            num_workers=num_worker,\n","            pin_memory=True,\n","        )\n","\n","\n","        # Intialize loss function\n","        loss_function = TripletMarginLoss(\n","            margin=config[\"margin\"],\n","            sampling_type=config[\"sampling_type\"]\n","        )\n","\n","\n","    elif config[\"loss\"] == \"proxy_nca\":\n","        # Initialize train loader for proxy-nca loss\n","        batch_size: int = config[\"batch_size\"]\n","        train_loader = DataLoader(\n","            train_set,\n","            config[\"batch_size\"],\n","            shuffle=True,\n","            num_workers=num_worker,\n","            pin_memory=True,\n","        )\n","\n","\n","        loss_function = ProxyNCALoss(\n","            n_classes=len(train_set.classes),\n","            embedding_size=config[\"embedding_size\"],\n","            embedding_scale=config[\"embedding_scale\"],\n","            proxy_scale=config[\"proxy_scale\"],\n","            smoothing_factor=config[\"smoothing_factor\"],\n","            device=device\n","        )\n","\n","    elif config[\"loss\"] == \"proxy_anchor\":\n","        # Intialize train loader for proxy-anchor loss\n","        batch_size: int = config[\"batch_size\"]\n","        train_loader = DataLoader(\n","            train_set,\n","            config[\"batch_size\"],\n","            shuffle=True,\n","            num_workers=config[\"n_workers\"],\n","            pin_memory=True,\n","        )\n","\n","\n","        loss_function = ProxyAnchorLoss(\n","            n_classes=len(train_set.classes),\n","            embedding_size=config[\"embedding_size\"],\n","            margin=config[\"margin\"],\n","            alpha=config[\"alpha\"],\n","            device=device\n","        )\n","\n","    elif config[\"loss\"] == \"soft_triple\":\n","        # Intialize train loader for proxy-anchor loss\n","        batch_size: int = config[\"batch_size\"]\n","        train_loader = DataLoader(\n","            train_set,\n","            config[\"batch_size\"],\n","            shuffle=True,\n","            num_workers=config[\"n_workers\"],\n","            pin_memory=True,\n","        )\n","\n","        loss_function = SoftTripleLoss(\n","            n_classes=len(train_set.classes),\n","            embedding_size=config[\"embedding_size\"],\n","            n_centers_per_class=config[\"n_centers_per_class\"],\n","            lambda_=config[\"lambda\"],\n","            gamma=config[\"gamma\"],\n","            tau=config[\"tau\"],\n","            margin=config[\"margin\"],\n","            device=device\n","        )\n","    else:\n","        raise Exception(\"Only the following losses is supported: \"\n","                        \"['tripletloss', 'proxy_nca', 'proxy_anchor', 'soft_triple']. \"\n","                        f\"Got {config['loss']}\")\n","\n","\n","    # Initialize test transforms\n","    transform_test = transforms.Compose([\n","        transforms.Resize((config[\"image_size\"], config[\"image_size\"])),\n","        transforms.CenterCrop((224, 224)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","    ])\n","\n","\n","\n","    # Initialize test set and test loader\n","    test_dataset = Dataset(val_dir, transform=transform_test)\n","    test_loader = DataLoader(\n","        test_dataset, batch_size,\n","        shuffle=False,\n","        num_workers=num_worker,\n","    )\n","\n","\n","    # Initialize reference set and reference loader\n","    reference_set = Dataset(train_dir, transform=transform_test)\n","\n","    n_samples_per_reference_class: int = config[\"n_samples_per_reference_class\"]\n","    if n_samples_per_reference_class > 0:\n","        reference_set = get_subset_from_dataset(reference_set, n_samples_per_reference_class)\n","\n","    reference_loader = DataLoader(reference_set, batch_size, shuffle=False, num_workers=num_worker)\n","\n","\n","\n","    # Initialize checkpointing directory\n","    checkpoint_dir: str = os.path.join(config[\"checkpoint_root_dir\"], CURRENT_TIME)\n","    writer = SummaryWriter(log_dir=checkpoint_dir)\n","    logger.info(f\"Created checkpoint directory at: {checkpoint_dir}\")\n","\n","\n","    # Dictionary contains all metrics\n","    output_dict: Dict[str, Any] = {\n","        \"total_epoch\": config[\"n_epochs\"],\n","        \"current_epoch\": 0,\n","        \"current_iter\": 0,\n","        \"metrics\": {\n","            \"mean_average_precision\": 0.0,\n","            \"average_precision_at_1\": 0.0,\n","            \"average_precision_at_5\": 0.0,\n","            \"average_precision_at_10\": 0.0,\n","            \"top_1_accuracy\": 0.0,\n","            \"top_5_accuracy\": 0.0,\n","            \"normalized_mutual_information\": 0.0,\n","        }\n","    }\n","    # Start training and testing\n","    logger.info(\"Start training...\")\n","    for _ in range(1, config[\"n_epochs\"] + 1):\n","        output_dict = train_one_epoch(\n","            model, optimizer, loss_function,\n","            train_loader, test_loader, reference_loader,\n","            writer, device, config,\n","            checkpoint_dir,\n","            config['log_frequency'],\n","            config['validate_frequency'],\n","            output_dict\n","        )\n","    logger.info(f\"DONE TRAINING {config['n_epochs']} epochs\")\n","\n","\n","    # Visualize embeddings\n","    logger.info(\"Calculating train embeddings for visualization...\")\n","    log_embeddings_to_tensorboard(train_loader, model, device, writer, tag=\"train\")\n","    logger.info(\"Calculating reference embeddings for visualization...\")\n","    log_embeddings_to_tensorboard(reference_loader, model, device, writer, tag=\"reference\")\n","    logger.info(\"Calculating test embeddings for visualization...\")\n","    log_embeddings_to_tensorboard(test_loader, model, device, writer, tag=\"test\")\n","\n","\n","    # Visualize model's graph\n","    logger.info(\"Adding graph for visualization\")\n","    with torch.no_grad():\n","        dummy_input = torch.zeros(1, 3, config[\"image_size\"], config[\"image_size\"]).to(device)\n","        writer.add_graph(model.module.features, dummy_input)\n","\n","\n","    # Save all hyper-parameters and corresponding metrics\n","    logger.info(\"Saving all hyper-parameters\")\n","    writer.add_hparams(\n","        config,\n","        metric_dict={f\"hyperparams/{key}\": value for key, value in output_dict[\"metrics\"].items()}\n","    )\n","    with open(os.path.join(checkpoint_dir, \"output_dict.json\"), \"w\") as f:\n","        json.dump(output_dict, f, indent=4)\n","    logger.info(f\"Dumped output_dict.json at {checkpoint_dir}\")\n","\n","\n","    end = time.time()\n","    logger.info(f\"EVERYTHING IS DONE. Training time: {round(end - start, 2)} seconds\")\n","\n","\n","if __name__ == \"__main__\":\n","    main()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"metric_learning.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}